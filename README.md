# NLP_Final_Project_1
#### Implementing Word2Vec  
In this part you will implement the word2vec model and train your own word vectors with stochastic gradient descent (SGD). Numpy methods could be utilized to make your code both shorter and faster. The following requirements should be satisfied:

a) Negative sampling loss  
b) Implement the skip-gram model from scratch  
c) Train with real-data  
d) Show the result embeddings  
